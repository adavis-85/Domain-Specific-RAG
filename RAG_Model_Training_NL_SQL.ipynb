{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9871d2-8bda-416b-87bd-ddf70f9d60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback, TrainerCallback\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "DATA_PATH = \"sft_data/text_to_sql_sampled_balanced.jsonl\"\n",
    "OUTPUT_DIR = \"./flan-t5-sql-model\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "BATCH_SIZE = 5\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create it immediately\n",
    "progress_file = os.path.join(OUTPUT_DIR, \"training_progress.json\")\n",
    "\n",
    "# Batch processing settings\n",
    "BATCH_TRAINING_SIZE = 1000  # Train on 1000 examples at a time\n",
    "SAVE_EVERY_N_STEPS = 100  # Save checkpoint every 100 steps (very frequent)\n",
    "\n",
    "# Custom callback to save frequently\n",
    "class FrequentCheckpointCallback(TrainerCallback):\n",
    "    def __init__(self, save_steps):\n",
    "        self.save_steps = save_steps\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.save_steps == 0:\n",
    "            control.should_save = True\n",
    "        return control\n",
    "\n",
    "# Check GPU/CPU\n",
    "print(f\"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a8a98-9039-406c-86dd-747cf8ae34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or resume model and tokenizer\n",
    "checkpoint_dir = None\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    checkpoints = [f for f in os.listdir(OUTPUT_DIR) if f.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        checkpoint_dir = os.path.join(OUTPUT_DIR, latest_checkpoint)\n",
    "        print(f\"‚úÖ Found checkpoint: {checkpoint_dir}\")\n",
    "        print(\"Loading from checkpoint...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_dir)\n",
    "    else:\n",
    "        print(\"No checkpoint found. Loading base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "else:\n",
    "    print(\"No checkpoint found. Loading base model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load training data\n",
    "print(\"\\nLoading training data...\")\n",
    "data_list = []\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data_list)} total examples\")\n",
    "\n",
    "# ‚úÖ Show first record before shuffle\n",
    "print(\"\\nFirst record BEFORE shuffle:\")\n",
    "print(json.dumps(data_list[0], indent=2))\n",
    "\n",
    "# ‚úÖ Shuffle the data\n",
    "import random\n",
    "random.seed(42)  # optional, for reproducibility\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# ‚úÖ Show first record after shuffle\n",
    "print(\"\\nFirst record AFTER shuffle:\")\n",
    "print(json.dumps(data_list[0], indent=2))\n",
    "\n",
    "print(\"\\n‚úÖ Data has been shuffled randomly.\\n\")\n",
    "print(f\"Loaded {len(data_list)} total examples\")\n",
    "\n",
    "# Analyze SQL query lengths to determine MAX_TARGET_LENGTH\n",
    "print(\"\\n--- Analyzing SQL Query Lengths ---\")\n",
    "sql_lengths = []\n",
    "for item in data_list:\n",
    "    sql = item[\"output\"]\n",
    "    # Use tokenizer to get accurate token count\n",
    "    tokens = len(tokenizer.encode(sql))\n",
    "    sql_lengths.append(tokens)\n",
    "\n",
    "avg_length = sum(sql_lengths) / len(sql_lengths)\n",
    "max_length = max(sql_lengths)\n",
    "min_length = min(sql_lengths)\n",
    "percentile_95 = sorted(sql_lengths)[int(len(sql_lengths) * 0.95)]\n",
    "percentile_99 = sorted(sql_lengths)[int(len(sql_lengths) * 0.99)]\n",
    "\n",
    "print(f\"Average SQL length: {avg_length:.1f} tokens\")\n",
    "print(f\"Min SQL length: {min_length} tokens\")\n",
    "print(f\"Max SQL length: {max_length} tokens\")\n",
    "print(f\"95th percentile: {percentile_95} tokens\")\n",
    "print(f\"99th percentile: {percentile_99} tokens\")\n",
    "\n",
    "# Set MAX_TARGET_LENGTH based on 99th percentile\n",
    "MAX_TARGET_LENGTH = min(percentile_99 + 20, 512)\n",
    "print(f\"Setting MAX_TARGET_LENGTH to: {MAX_TARGET_LENGTH} tokens\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a06371-c919-4147-a47a-78bcfc61f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/val/test FIRST (before batching)\n",
    "print(\"Creating train/val/test split...\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 90% train, 10% test\n",
    "train_val_data, test_data = train_test_split(data_list, test_size=0.1, random_state=42)\n",
    "\n",
    "# Second split: ~89% train, ~11% val (of the 90%)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.1111, random_state=42)\n",
    "\n",
    "print(f\"Total training examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")\n",
    "print(f\"Test examples: {len(test_data)}\")\n",
    "\n",
    "# Calculate number of batches needed\n",
    "num_batches = (len(train_data) + BATCH_TRAINING_SIZE - 1) // BATCH_TRAINING_SIZE\n",
    "print(f\"\\nWill train in {num_batches} batches of ~{BATCH_TRAINING_SIZE} examples each\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c9e98-9d7f-4d70-a772-9089992a1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset function\n",
    "def create_dataset(data_list):\n",
    "    return Dataset.from_dict({\n",
    "        \"instruction\": [d[\"instruction\"] for d in data_list],\n",
    "        \"input\": [d.get(\"input\", \"\") for d in data_list],\n",
    "        \"output\": [d[\"output\"] for d in data_list]\n",
    "    })\n",
    "\n",
    "# Add these helper functions at the top of your code\n",
    "def encode_sql_for_training(sql):\n",
    "    \"\"\"Encode SQL operators that conflict with tokenizer special tokens\"\"\"\n",
    "    sql = sql.replace('<=', ' LESS_EQUAL ')\n",
    "    sql = sql.replace('>=', ' GREATER_EQUAL ')\n",
    "    sql = sql.replace('<', ' LESS_THAN ')\n",
    "    sql = sql.replace('>', ' GREATER_THAN ')\n",
    "    return ' '.join(sql.split())  # Normalize spaces\n",
    "\n",
    "def decode_sql_from_model(sql):\n",
    "    \"\"\"Decode SQL operators back from training format\"\"\"\n",
    "    sql = sql.replace('LESS_EQUAL', '<=')\n",
    "    sql = sql.replace('GREATER_EQUAL', '>=')\n",
    "    sql = sql.replace('LESS_THAN', '<')\n",
    "    sql = sql.replace('GREATER_THAN', '>')\n",
    "    return sql\n",
    "\n",
    "# Update preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Translate this to SQL: {instr} {inp}\".strip() \n",
    "              for instr, inp in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    \n",
    "    # Encode outputs to avoid tokenizer issues with < and >\n",
    "    outputs = [encode_sql_for_training(sql) for sql in examples[\"output\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(outputs, max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fff7e-fe05-4f4c-97e8-98aa301c613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##if training stopped by user\n",
    "import json\n",
    "import os\n",
    "\n",
    "progress_file = \"./training_progress.json\"\n",
    "\n",
    "# Set which batch you want to start from\n",
    "restart_batch = 5  # Will start training batch 5 (skipping batches 1-4)\n",
    "\n",
    "with open(progress_file, 'w') as f:\n",
    "    json.dump({\n",
    "        \"completed_batches\": restart_batch - 1,  # -1 because we count completed batches\n",
    "        \"current_batch_started\": False,\n",
    "        \"all_losses\": [],\n",
    "        \"total_batches\": 8,  # Or however many batches you have\n",
    "        \"last_batch_max_checkpoint\": 0\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Training will restart from batch {restart_batch}\")\n",
    "print(\"Now run your training code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5d12d-d5bc-4365-b507-503ab6834114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation and test datasets (these stay constant)\n",
    "print(\"Preparing validation and test datasets...\")\n",
    "val_dataset = create_dataset(val_data)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "test_dataset = create_dataset(test_data)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n",
    "# Training loop - process data in batches\n",
    "all_losses = []\n",
    "completed_batches = 0\n",
    "current_batch_started = False\n",
    "last_batch_max_checkpoint = 0  # Track checkpoint numbers per batch\n",
    "\n",
    "\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as f:\n",
    "        progress = json.load(f)\n",
    "        completed_batches = progress.get(\"completed_batches\", 0)\n",
    "        current_batch_started = progress.get(\"current_batch_started\", False)\n",
    "        all_losses = progress.get(\"all_losses\", [])\n",
    "        last_batch_max_checkpoint = progress.get(\"last_batch_max_checkpoint\", 0)\n",
    "    print(f\"‚úÖ Resuming from batch {completed_batches + 1}/{num_batches}\\n\")\n",
    "\n",
    "for batch_idx in range(completed_batches, num_batches):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING BATCH {batch_idx + 1}/{num_batches}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Get batch of training data\n",
    "    start_idx = batch_idx * BATCH_TRAINING_SIZE\n",
    "    end_idx = min(start_idx + BATCH_TRAINING_SIZE, len(train_data))\n",
    "    batch_train_data = train_data[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Training on examples {start_idx} to {end_idx} ({len(batch_train_data)} examples)\")\n",
    "    \n",
    "    # Create training dataset for this batch\n",
    "    train_dataset_batch = create_dataset(batch_train_data)\n",
    "    train_dataset_batch = train_dataset_batch.map(preprocess_function, batched=True, remove_columns=train_dataset_batch.column_names)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    # Determine if we're resuming or starting fresh\n",
    "    resume_from_checkpoint = None\n",
    "    \n",
    "    # CASE 1: We're resuming the SAME batch after a crash\n",
    "    if current_batch_started and batch_idx == completed_batches:\n",
    "        # Look for checkpoint to resume\n",
    "        if os.path.exists(OUTPUT_DIR):\n",
    "            checkpoints = [f for f in os.listdir(OUTPUT_DIR) if f.startswith(\"checkpoint-\")]\n",
    "            if checkpoints:\n",
    "                latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "                resume_from_checkpoint = os.path.join(OUTPUT_DIR, latest_checkpoint)\n",
    "                print(f\"üìå Resuming batch {batch_idx + 1} from checkpoint: {resume_from_checkpoint}\")\n",
    "    \n",
    "    # CASE 2: Starting a NEW batch\n",
    "    else:\n",
    "        # Delete old checkpoints from PREVIOUS batch\n",
    "        if os.path.exists(OUTPUT_DIR) and batch_idx > 0:\n",
    "            import shutil\n",
    "            checkpoints = [f for f in os.listdir(OUTPUT_DIR) if f.startswith(\"checkpoint-\")]\n",
    "            for ckpt in checkpoints:\n",
    "                ckpt_num = int(ckpt.split(\"-\")[1])\n",
    "                # Only delete if it's from the previous batch\n",
    "                if ckpt_num <= last_batch_max_checkpoint:\n",
    "                    ckpt_path = os.path.join(OUTPUT_DIR, ckpt)\n",
    "                    try:\n",
    "                        shutil.rmtree(ckpt_path)\n",
    "                        print(f\"üßπ Cleaned up old checkpoint: {ckpt}\")\n",
    "                    except:\n",
    "                        pass\n",
    "        print(f\"üÜï Starting fresh training for batch {batch_idx + 1}\")\n",
    "    \n",
    "    # Mark that we've started this batch (for crash recovery)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump({\n",
    "            \"completed_batches\": completed_batches,\n",
    "            \"current_batch_started\": True,\n",
    "            \"all_losses\": all_losses,\n",
    "            \"total_batches\": num_batches,\n",
    "            \"last_batch_max_checkpoint\": last_batch_max_checkpoint\n",
    "        }, f)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=SAVE_EVERY_N_STEPS,\n",
    "        load_best_model_at_end=False,\n",
    "        logging_steps=50,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_batch,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[FrequentCheckpointCallback(save_steps=SAVE_EVERY_N_STEPS)]\n",
    "    )\n",
    "    \n",
    "    # Train this batch\n",
    "    print(f\"Starting training for batch {batch_idx + 1}...\")\n",
    "    \n",
    "    try:\n",
    "        train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "        \n",
    "        # Find max checkpoint number from this batch\n",
    "        if os.path.exists(OUTPUT_DIR):\n",
    "            checkpoints = [f for f in os.listdir(OUTPUT_DIR) if f.startswith(\"checkpoint-\")]\n",
    "            if checkpoints:\n",
    "                last_batch_max_checkpoint = max([int(c.split(\"-\")[1]) for c in checkpoints])\n",
    "        \n",
    "        # Batch completed successfully\n",
    "        completed_batches = batch_idx + 1\n",
    "        \n",
    "        # Collect losses\n",
    "        logs = trainer.state.log_history\n",
    "        for log in logs:\n",
    "            if \"loss\" in log or \"eval_loss\" in log:\n",
    "                all_losses.append(log)\n",
    "        \n",
    "        # Save progress - mark batch as complete, reset started flag\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        with open(progress_file, 'w') as f:\n",
    "            json.dump({\n",
    "                \"completed_batches\": completed_batches,\n",
    "                \"current_batch_started\": False,\n",
    "                \"all_losses\": all_losses,\n",
    "                \"total_batches\": num_batches,\n",
    "                \"last_batch_max_checkpoint\": last_batch_max_checkpoint\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Batch {batch_idx + 1}/{num_batches} complete!\")\n",
    "        print(f\"Progress saved. Safe to stop/restart anytime.\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "\n",
    "        \n",
    "        print(f\"\\n‚ùå Error during training: {e}\")\n",
    "        print(f\"Progress saved. Restart to resume from checkpoint.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e380c9a-e167-4462-acf0-f25e4b624484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration to train everything all at once\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "DATA_PATH = \"sft_data/text_to_sql_sampled_balanced.jsonl\"\n",
    "OUTPUT_DIR = \"./flan-t5-sql-model\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Load all data at once\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    all_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Split\n",
    "random.shuffle(all_data)\n",
    "train_size = int(0.8 * len(all_data))\n",
    "val_size = int(0.1 * len(all_data))\n",
    "\n",
    "train_data = all_data[:train_size]\n",
    "val_data = all_data[train_size:train_size+val_size]\n",
    "test_data = all_data[train_size+val_size:]\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_data)\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "\n",
    "val_dataset = create_dataset(val_data)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "\n",
    "test_dataset = create_dataset(test_data)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"‚úÖ Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b03b8a-c5f0-4aab-a33a-4fb06ece81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL BATCHES COMPLETE!\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Save final model\n",
    "print(\"Saving final model...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"‚úÖ Model saved to {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"--- Evaluating on Test Set ---\")\n",
    "\n",
    "# Create new training args for evaluation only\n",
    "eval_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"no\",  # No eval strategy needed for just testing\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"Test Loss: {test_results['eval_loss']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299192fc-9747-4809-b522-d4d0e5b56c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all losses\n",
    "print(\"--- Generating Loss Plots ---\")\n",
    "train_losses = [log[\"loss\"] for log in all_losses if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in all_losses if \"eval_loss\" in log]\n",
    "\n",
    "if train_losses:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, marker='o', linewidth=1, markersize=3)\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    if eval_losses:\n",
    "        plt.plot(eval_losses, marker='s', linewidth=2, markersize=6, color='orange')\n",
    "        plt.xlabel('Evaluation Step')\n",
    "        plt.ylabel('Validation Loss')\n",
    "        plt.title('Validation Loss Over Time')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b46b238-5833-4688-834c-093160e95da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train test adn val set for uniqueness of query.  adress with data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69c99c-7723-40fa-88a0-33371ddf38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "print(\"\\n--- Sample Predictions from Test Set ---\")\n",
    "import random\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(\"customer_data.db\")\n",
    "\n",
    "random.seed(42)\n",
    "sample_indices = random.sample(range(len(test_dataset)), min(10, len(test_dataset)))\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = test_dataset[idx]\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]]).to(model.device)\n",
    "    outputs = model.generate(input_ids, max_length=MAX_TARGET_LENGTH)\n",
    "    predicted_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    actual_sql = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "    \n",
    "    # DECODE the operators back to SQL format\n",
    "    predicted_sql = decode_sql_from_model(predicted_sql)\n",
    "    actual_sql = decode_sql_from_model(actual_sql)\n",
    "    \n",
    "    nl_question = test_data[idx][\"instruction\"]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {idx}:\")\n",
    "    print(f\"\\nNatural Language: {nl_question}\")\n",
    "    print(f\"\\nPredicted SQL: {predicted_sql}\")\n",
    "    print(f\"\\nActual SQL:    {actual_sql}\")\n",
    "    # Try to execute predicted SQL\n",
    "    try:\n",
    "        result_df = pd.read_sql_query(predicted_sql, conn)\n",
    "        print(f\"\\n‚úÖ Predicted Query Results ({len(result_df)} rows):\")\n",
    "        if len(result_df) > 0:\n",
    "            # Show first 5 rows\n",
    "            print(result_df.head().to_string(index=False))\n",
    "            if len(result_df) > 5:\n",
    "                print(f\"... (showing 5 of {len(result_df)} rows)\")\n",
    "        else:\n",
    "            print(\"  No results returned\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Predicted query failed: {str(e)}\")\n",
    "    \n",
    "    # Try to execute actual SQL for comparison\n",
    "    try:\n",
    "        actual_result_df = pd.read_sql_query(actual_sql, conn)\n",
    "        print(f\"\\nüìã Actual Query Results ({len(actual_result_df)} rows):\")\n",
    "        if len(actual_result_df) > 0:\n",
    "            print(actual_result_df.head().to_string(index=False))\n",
    "            if len(actual_result_df) > 5:\n",
    "                print(f\"... (showing 5 of {len(actual_result_df)} rows)\")\n",
    "        else:\n",
    "            print(\"  No results returned\")\n",
    "        \n",
    "        # Compare results\n",
    "        if len(result_df) == len(actual_result_df) and result_df.equals(actual_result_df):\n",
    "            print(\"\\n‚úÖ MATCH: Predicted and actual queries return identical results!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  MISMATCH: Different results (Predicted: {len(result_df)} rows, Actual: {len(actual_result_df)} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Actual query failed: {str(e)}\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "conn.close()\n",
    "print(\"\\n‚úÖ All training complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30466c-1fc6-4dcc-b61d-06d70b5222ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions (only for queries containing < or >)\n",
    "print(\"\\n--- Sample Predictions from Test Set (Filtered for < or >) ---\")\n",
    "import random\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(\"customer_data.db\")\n",
    "\n",
    "# Filter indices to only include SQL queries with < or >\n",
    "filtered_indices = [\n",
    "    i for i in range(len(test_dataset))\n",
    "    if \"<\" in decode_sql_from_model(tokenizer.decode(test_dataset[i][\"labels\"], skip_special_tokens=True))\n",
    "    or \">\" in decode_sql_from_model(tokenizer.decode(test_dataset[i][\"labels\"], skip_special_tokens=True))\n",
    "]\n",
    "\n",
    "if not filtered_indices:\n",
    "    print(\"‚ö†Ô∏è No queries with < or > found in test set.\")\n",
    "else:\n",
    "    random.seed(42)\n",
    "    sample_indices = random.sample(filtered_indices, min(10, len(filtered_indices)))\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        sample = test_dataset[idx]\n",
    "        input_ids = torch.tensor([sample[\"input_ids\"]]).to(model.device)\n",
    "        outputs = model.generate(input_ids, max_length=MAX_TARGET_LENGTH)\n",
    "        predicted_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        actual_sql = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "        # Decode operators back to SQL format\n",
    "        predicted_sql = decode_sql_from_model(predicted_sql)\n",
    "        actual_sql = decode_sql_from_model(actual_sql)\n",
    "\n",
    "        nl_question = test_data[idx][\"instruction\"]\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Example {idx}:\")\n",
    "        print(f\"\\nNatural Language: {nl_question}\")\n",
    "        print(f\"\\nPredicted SQL: {predicted_sql}\")\n",
    "        print(f\"\\nActual SQL:    {actual_sql}\")\n",
    "\n",
    "        # Try to execute predicted SQL\n",
    "        try:\n",
    "            result_df = pd.read_sql_query(predicted_sql, conn)\n",
    "            print(f\"\\n‚úÖ Predicted Query Results ({len(result_df)} rows):\")\n",
    "            if len(result_df) > 0:\n",
    "                print(result_df.head().to_string(index=False))\n",
    "                if len(result_df) > 5:\n",
    "                    print(f\"... (showing 5 of {len(result_df)} rows)\")\n",
    "            else:\n",
    "                print(\"  No results returned\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Predicted query failed: {str(e)}\")\n",
    "\n",
    "        # Try to execute actual SQL for comparison\n",
    "        try:\n",
    "            actual_result_df = pd.read_sql_query(actual_sql, conn)\n",
    "            print(f\"\\nüìã Actual Query Results ({len(actual_result_df)} rows):\")\n",
    "            if len(actual_result_df) > 0:\n",
    "                print(actual_result_df.head().to_string(index=False))\n",
    "                if len(actual_result_df) > 5:\n",
    "                    print(f\"... (showing 5 of {len(actual_result_df)} rows)\")\n",
    "            else:\n",
    "                print(\"  No results returned\")\n",
    "\n",
    "            # Compare results\n",
    "            if len(result_df) == len(actual_result_df) and result_df.equals(actual_result_df):\n",
    "                print(\"\\n‚úÖ MATCH: Predicted and actual queries return identical results!\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  MISMATCH: Different results (Predicted: {len(result_df)} rows, Actual: {len(actual_result_df)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Actual query failed: {str(e)}\")\n",
    "\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "conn.close()\n",
    "print(\"\\n‚úÖ All training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a485aa4-923d-41d4-a856-f144ec3e7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Test Set Analysis with Query Type Labeling\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE TEST SET ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import random\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(\"customer_data.db\")\n",
    "\n",
    "# Helper function to categorize queries\n",
    "def categorize_query(sql):\n",
    "    \"\"\"Categorize SQL query by type and complexity\"\"\"\n",
    "    sql_upper = sql.upper()\n",
    "    categories = []\n",
    "    \n",
    "    # Main query types\n",
    "    if 'WITH' in sql_upper:\n",
    "        categories.append('CTE')\n",
    "    if 'CASE WHEN' in sql_upper:\n",
    "        categories.append('CASE_WHEN')\n",
    "    if 'JOIN' in sql_upper:\n",
    "        categories.append('JOIN')\n",
    "    if 'GROUP BY' in sql_upper:\n",
    "        categories.append('GROUP_BY')\n",
    "    if 'ORDER BY' in sql_upper:\n",
    "        categories.append('ORDER_BY')\n",
    "    if 'ROW_NUMBER()' in sql_upper or 'PARTITION BY' in sql_upper:\n",
    "        categories.append('WINDOW_FUNCTION')\n",
    "    \n",
    "    # Aggregation types\n",
    "    if 'AVG(' in sql_upper:\n",
    "        categories.append('AVG')\n",
    "    if 'MAX(' in sql_upper:\n",
    "        categories.append('MAX')\n",
    "    if 'MIN(' in sql_upper:\n",
    "        categories.append('MIN')\n",
    "    if 'SUM(' in sql_upper:\n",
    "        categories.append('SUM')\n",
    "    if 'COUNT(' in sql_upper:\n",
    "        categories.append('COUNT')\n",
    "    \n",
    "    # Operators\n",
    "    if 'LESS_THAN' in sql or '<' in sql:\n",
    "        categories.append('LESS_THAN_OP')\n",
    "    if 'GREATER_THAN' in sql or '>' in sql:\n",
    "        categories.append('GREATER_THAN_OP')\n",
    "    if 'LESS_EQUAL' in sql or '<=' in sql:\n",
    "        categories.append('LESS_EQUAL_OP')\n",
    "    if 'GREATER_EQUAL' in sql or '>=' in sql:\n",
    "        categories.append('GREATER_EQUAL_OP')\n",
    "    \n",
    "    # Simple queries\n",
    "    if len(categories) == 0:\n",
    "        if 'SELECT * FROM' in sql_upper and 'WHERE' in sql_upper:\n",
    "            categories.append('SIMPLE_FILTER')\n",
    "        elif 'SELECT DISTINCT' in sql_upper:\n",
    "            categories.append('DISTINCT')\n",
    "        elif 'SELECT COUNT(*)' in sql_upper:\n",
    "            categories.append('SIMPLE_COUNT')\n",
    "        else:\n",
    "            categories.append('SIMPLE_SELECT')\n",
    "    \n",
    "    # Complexity level\n",
    "    complexity = 'SIMPLE'\n",
    "    if len(categories) >= 3:\n",
    "        complexity = 'COMPLEX'\n",
    "    elif len(categories) >= 2:\n",
    "        complexity = 'MEDIUM'\n",
    "    \n",
    "    return categories, complexity\n",
    "\n",
    "# Helper function to check if queries match semantically\n",
    "def queries_match_semantically(predicted, actual):\n",
    "    \"\"\"Check if two queries are semantically equivalent\"\"\"\n",
    "    # Exact match\n",
    "    if predicted.strip() == actual.strip():\n",
    "        return True, \"EXACT_MATCH\"\n",
    "    \n",
    "    # Normalize whitespace and compare\n",
    "    pred_normalized = ' '.join(predicted.split())\n",
    "    actual_normalized = ' '.join(actual.split())\n",
    "    if pred_normalized == actual_normalized:\n",
    "        return True, \"WHITESPACE_DIFF\"\n",
    "    \n",
    "    return False, \"DIFFERENT\"\n",
    "\n",
    "# Helper function to try executing and compare results\n",
    "def execute_and_compare(predicted_sql, actual_sql, conn):\n",
    "    \"\"\"Execute both queries and compare results\"\"\"\n",
    "    try:\n",
    "        pred_result = pd.read_sql_query(predicted_sql, conn)\n",
    "        pred_success = True\n",
    "    except Exception as e:\n",
    "        pred_result = None\n",
    "        pred_success = False\n",
    "        pred_error = str(e)\n",
    "    \n",
    "    try:\n",
    "        actual_result = pd.read_sql_query(actual_sql, conn)\n",
    "        actual_success = True\n",
    "    except Exception as e:\n",
    "        actual_result = None\n",
    "        actual_success = False\n",
    "        actual_error = str(e)\n",
    "    \n",
    "    if pred_success and actual_success:\n",
    "        # Compare results\n",
    "        if len(pred_result) == len(actual_result):\n",
    "            if pred_result.equals(actual_result):\n",
    "                return \"RESULTS_MATCH\", True\n",
    "            else:\n",
    "                return \"RESULTS_DIFFER\", False\n",
    "        else:\n",
    "            return f\"ROW_COUNT_DIFF ({len(pred_result)} vs {len(actual_result)})\", False\n",
    "    elif not pred_success and not actual_success:\n",
    "        return \"BOTH_FAILED\", False\n",
    "    elif not pred_success:\n",
    "        return f\"PRED_FAILED: {pred_error[:50]}\", False\n",
    "    else:\n",
    "        return f\"ACTUAL_FAILED: {actual_error[:50]}\", False\n",
    "\n",
    "# Analyze all test samples\n",
    "random.seed(42)\n",
    "all_indices = list(range(len(test_dataset)))\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nAnalyzing all test samples...\")\n",
    "for idx in all_indices:\n",
    "    sample = test_dataset[idx]\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]]).to(model.device)\n",
    "    outputs = model.generate(input_ids, max_length=MAX_TARGET_LENGTH)\n",
    "    \n",
    "    predicted_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    actual_sql = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "    \n",
    "    # Decode operators\n",
    "    predicted_sql = decode_sql_from_model(predicted_sql)\n",
    "    actual_sql = decode_sql_from_model(actual_sql)\n",
    "    \n",
    "    nl_question = test_data[idx][\"instruction\"]\n",
    "    \n",
    "    # Categorize\n",
    "    actual_categories, actual_complexity = categorize_query(actual_sql)\n",
    "    predicted_categories, predicted_complexity = categorize_query(predicted_sql)\n",
    "    \n",
    "    # Check if match\n",
    "    is_match, match_type = queries_match_semantically(predicted_sql, actual_sql)\n",
    "    \n",
    "    # Execute and compare\n",
    "    exec_result, results_match = execute_and_compare(predicted_sql, actual_sql, conn)\n",
    "    \n",
    "    results.append({\n",
    "        'idx': idx,\n",
    "        'question': nl_question,\n",
    "        'predicted_sql': predicted_sql,\n",
    "        'actual_sql': actual_sql,\n",
    "        'actual_categories': actual_categories,\n",
    "        'actual_complexity': actual_complexity,\n",
    "        'predicted_categories': predicted_categories,\n",
    "        'is_exact_match': is_match,\n",
    "        'match_type': match_type,\n",
    "        'exec_result': exec_result,\n",
    "        'results_match': results_match\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total = len(df_results)\n",
    "exact_matches = df_results['is_exact_match'].sum()\n",
    "result_matches = df_results['results_match'].sum()\n",
    "\n",
    "print(f\"\\nTotal test samples: {total}\")\n",
    "print(f\"Exact SQL matches: {exact_matches} ({100*exact_matches/total:.1f}%)\")\n",
    "print(f\"Semantic matches (same results): {result_matches} ({100*result_matches/total:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ACCURACY BY QUERY COMPLEXITY\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for complexity in ['SIMPLE', 'MEDIUM', 'COMPLEX']:\n",
    "    subset = df_results[df_results['actual_complexity'] == complexity]\n",
    "    if len(subset) > 0:\n",
    "        acc = subset['results_match'].sum()\n",
    "        print(f\"{complexity:12} : {acc}/{len(subset)} ({100*acc/len(subset):.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ACCURACY BY QUERY TYPE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Get all unique categories\n",
    "all_categories = set()\n",
    "for cats in df_results['actual_categories']:\n",
    "    all_categories.update(cats)\n",
    "\n",
    "category_stats = []\n",
    "for cat in sorted(all_categories):\n",
    "    subset = df_results[df_results['actual_categories'].apply(lambda x: cat in x)]\n",
    "    if len(subset) > 0:\n",
    "        correct = subset['results_match'].sum()\n",
    "        total_cat = len(subset)\n",
    "        accuracy = 100 * correct / total_cat\n",
    "        category_stats.append({\n",
    "            'category': cat,\n",
    "            'total': total_cat,\n",
    "            'correct': correct,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "\n",
    "df_cat_stats = pd.DataFrame(category_stats).sort_values('accuracy')\n",
    "print(df_cat_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOWEST PERFORMING QUERY TYPES (Need More Training Data)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "worst_performing = df_cat_stats[df_cat_stats['accuracy'] < 85].sort_values('accuracy')\n",
    "if len(worst_performing) > 0:\n",
    "    print(worst_performing.to_string(index=False))\n",
    "    print(\"\\n‚ö†Ô∏è  Focus training data generation on these query types!\")\n",
    "else:\n",
    "    print(\"‚úÖ All query types performing above 85%!\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SAMPLE FAILURES BY TYPE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Show examples of failures for each low-performing category\n",
    "for _, row in worst_performing.head(5).iterrows():\n",
    "    cat = row['category']\n",
    "    print(f\"\\nüìå Category: {cat} (Accuracy: {row['accuracy']:.1f}%)\")\n",
    "    \n",
    "    # Get a failure example\n",
    "    failures = df_results[\n",
    "        (df_results['actual_categories'].apply(lambda x: cat in x)) & \n",
    "        (~df_results['results_match'])\n",
    "    ]\n",
    "    \n",
    "    if len(failures) > 0:\n",
    "        example = failures.iloc[0]\n",
    "        print(f\"   Question: {example['question'][:80]}...\")\n",
    "        print(f\"   Expected: {example['actual_sql'][:100]}...\")\n",
    "        print(f\"   Got:      {example['predicted_sql'][:100]}...\")\n",
    "        print(f\"   Issue:    {example['exec_result']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED FAILURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "failures = df_results[~df_results['results_match']].copy()\n",
    "print(f\"\\nTotal failures: {len(failures)}\")\n",
    "\n",
    "if len(failures) > 0:\n",
    "    print(\"\\nFailure reasons:\")\n",
    "    failure_reasons = failures['exec_result'].value_counts()\n",
    "    for reason, count in failure_reasons.items():\n",
    "        print(f\"  {reason}: {count}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"SHOWING 10 RANDOM FAILURE EXAMPLES\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    sample_failures = failures.sample(min(10, len(failures)), random_state=42)\n",
    "    \n",
    "    for i, (_, row) in enumerate(sample_failures.iterrows(), 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FAILURE EXAMPLE {i} (Categories: {', '.join(row['actual_categories'])})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Question: {row['question']}\")\n",
    "        print(f\"\\nExpected SQL:\\n{row['actual_sql']}\")\n",
    "        print(f\"\\nPredicted SQL:\\n{row['predicted_sql']}\")\n",
    "        print(f\"\\nIssue: {row['exec_result']}\")\n",
    "        print(f\"Complexity: {row['actual_complexity']}\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Save detailed results to CSV\n",
    "output_csv = \"test_set_analysis.csv\"\n",
    "df_results.to_csv(output_csv, index=False)\n",
    "print(f\"\\n‚úÖ Detailed results saved to {output_csv}\")\n",
    "\n",
    "# Save category statistics\n",
    "cat_stats_csv = \"category_performance.csv\"\n",
    "df_cat_stats.to_csv(cat_stats_csv, index=False)\n",
    "print(f\"‚úÖ Category statistics saved to {cat_stats_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0059e9-26e8-4e2b-ae20-767e7550cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# RETRAIN EXISTING MODEL WITH ENHANCED DATA\n",
    "# ==========================================\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback, TrainerCallback\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "MODEL_DIR = \"./flan-t5-sql-model\"  # Your existing trained model\n",
    "ENHANCED_DATA_PATH = \"sft_data/text_to_sql_enhanced.jsonl\"\n",
    "OUTPUT_DIR = \"./flan-t5-sql-model-v2\"  # Save improved model here\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5  # Fewer epochs since continuing training\n",
    "LEARNING_RATE = 1e-4  # Lower LR for fine-tuning\n",
    "SAVE_EVERY_N_STEPS = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load your EXISTING trained model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING EXISTING TRAINED MODEL\")\n",
    "print(\"=\"*80)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "model.to(device)\n",
    "print(f\"‚úÖ Loaded model from {MODEL_DIR}\")\n",
    "\n",
    "# Load enhanced dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING ENHANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "with open(ENHANCED_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    all_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total samples: {len(all_data)}\")\n",
    "\n",
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "random.shuffle(all_data)\n",
    "train_size = int(0.8 * len(all_data))\n",
    "val_size = int(0.1 * len(all_data))\n",
    "\n",
    "train_data = all_data[:train_size]\n",
    "val_data = all_data[train_size:train_size+val_size]\n",
    "test_data = all_data[train_size+val_size:]\n",
    "\n",
    "print(f\"  Train: {len(train_data)} samples\")\n",
    "print(f\"  Val: {len(val_data)} samples\")\n",
    "print(f\"  Test: {len(test_data)} samples\")\n",
    "\n",
    "# Helper functions\n",
    "def create_dataset(data_list):\n",
    "    return Dataset.from_dict({\n",
    "        \"instruction\": [d[\"instruction\"] for d in data_list],\n",
    "        \"input\": [d.get(\"input\", \"\") for d in data_list],\n",
    "        \"output\": [d[\"output\"] for d in data_list]\n",
    "    })\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Translate this to SQL: {instr} {inp}\".strip() \n",
    "              for instr, inp in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(examples[\"output\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARING DATASETS\")\n",
    "print(\"=\"*80)\n",
    "train_dataset = create_dataset(train_data)\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "\n",
    "val_dataset = create_dataset(val_data)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "\n",
    "test_dataset = create_dataset(test_data)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "print(\"‚úÖ Datasets prepared\")\n",
    "\n",
    "# Callback for frequent checkpoints\n",
    "class FrequentCheckpointCallback(TrainerCallback):\n",
    "    def __init__(self, save_steps):\n",
    "        self.save_steps = save_steps\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.save_steps == 0:\n",
    "            control.should_save = True\n",
    "        return control\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",  # Changed from \"epoch\"\n",
    "    eval_steps=SAVE_EVERY_N_STEPS,  # Added this\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_EVERY_N_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[FrequentCheckpointCallback(save_steps=SAVE_EVERY_N_STEPS)]\n",
    ")\n",
    "\n",
    "# Train (continue training)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ STARTING CONTINUED TRAINING WITH ENHANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training on {len(train_dataset)} samples for {EPOCHS} epochs\")\n",
    "print(f\"Lower learning rate: {LEARNING_RATE} (for fine-tuning)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save final model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING ENHANCED MODEL\")\n",
    "print(\"=\"*80)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"‚úÖ Enhanced model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_results['eval_loss']:.4f}\")\n",
    "\n",
    "# Show training history\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "logs = trainer.state.log_history\n",
    "train_losses = [x.get('loss') for x in logs if 'loss' in x]\n",
    "eval_losses = [x.get('eval_loss') for x in logs if 'eval_loss' in x]\n",
    "\n",
    "if train_losses:\n",
    "    print(f\"Initial training loss: {train_losses[0]:.4f}\")\n",
    "    print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"Loss reduction: {train_losses[0] - train_losses[-1]:.4f}\")\n",
    "\n",
    "if eval_losses:\n",
    "    print(f\"\\nInitial validation loss: {eval_losses[0]:.4f}\")\n",
    "    print(f\"Final validation loss: {eval_losses[-1]:.4f}\")\n",
    "    print(f\"Val loss reduction: {eval_losses[0] - eval_losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nTest loss: {test_results['eval_loss']:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "if len(train_losses) > 0 and len(eval_losses) > 0:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "    plt.xlabel('Checkpoint')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(eval_losses, label='Validation Loss', marker='s', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n‚úÖ Training curves saved to {OUTPUT_DIR}/training_curves.png\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ RETRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Original model: {MODEL_DIR}\")\n",
    "print(f\"Enhanced model: {OUTPUT_DIR}\")\n",
    "print(f\"\\nExpected improvements:\")\n",
    "print(\"  ‚Ä¢ GREATER_EQUAL: 42% ‚Üí 75%+\")\n",
    "print(\"  ‚Ä¢ LESS_EQUAL: 67% ‚Üí 80%+\")\n",
    "print(\"  ‚Ä¢ CTE: 66% ‚Üí 78%+\")\n",
    "print(\"  ‚Ä¢ Overall: 90% ‚Üí 93%+\")\n",
    "print(\"\\nRun your test evaluation script to verify improvements!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000aca2c-af9b-4892-89d0-ae91b89654bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE TEST SET ANALYSIS - UPDATED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import random\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "MODEL_DIR = \"./flan-t5-sql-model-v2\"  # Your newly trained model\n",
    "TEST_DATA_PATH = \"sft_data/text_to_sql_enhanced.jsonl\"  # Your test data\n",
    "MAX_TARGET_LENGTH = 256\n",
    "MAX_INPUT_LENGTH = 512\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(\"customer_data.db\")\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nLoading updated model from {MODEL_DIR}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"‚úÖ Model loaded on {device}\")\n",
    "\n",
    "# Load test data\n",
    "print(f\"\\nLoading test data...\")\n",
    "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Use a subset for testing (or use your original test split)\n",
    "random.seed(42)\n",
    "random.shuffle(test_data)\n",
    "test_data = test_data[:200]  # Adjust size as needed\n",
    "print(f\"‚úÖ Using {len(test_data)} test samples\")\n",
    "\n",
    "# Prepare test dataset\n",
    "def create_dataset(data_list):\n",
    "    return Dataset.from_dict({\n",
    "        \"instruction\": [d[\"instruction\"] for d in data_list],\n",
    "        \"input\": [d.get(\"input\", \"\") for d in data_list],\n",
    "        \"output\": [d[\"output\"] for d in data_list]\n",
    "    })\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Translate this to SQL: {instr} {inp}\".strip() \n",
    "              for instr, inp in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(examples[\"output\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "test_dataset = create_dataset(test_data)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "# Decode SQL operators\n",
    "def decode_sql_from_model(sql):\n",
    "    \"\"\"Decode SQL operators back to standard form\"\"\"\n",
    "    sql = sql.replace(' LESS_EQUAL ', ' <= ')\n",
    "    sql = sql.replace(' GREATER_EQUAL ', ' >= ')\n",
    "    sql = sql.replace(' LESS_THAN ', ' < ')\n",
    "    sql = sql.replace(' GREATER_THAN ', ' > ')\n",
    "    return sql\n",
    "\n",
    "# Helper function to categorize queries\n",
    "def categorize_query(sql):\n",
    "    \"\"\"Categorize SQL query by type and complexity\"\"\"\n",
    "    sql_upper = sql.upper()\n",
    "    categories = []\n",
    "    \n",
    "    # Main query types\n",
    "    if 'WITH' in sql_upper:\n",
    "        categories.append('CTE')\n",
    "    if 'CASE WHEN' in sql_upper:\n",
    "        categories.append('CASE_WHEN')\n",
    "    if 'JOIN' in sql_upper:\n",
    "        categories.append('JOIN')\n",
    "    if 'GROUP BY' in sql_upper:\n",
    "        categories.append('GROUP_BY')\n",
    "    if 'ORDER BY' in sql_upper:\n",
    "        categories.append('ORDER_BY')\n",
    "    if 'ROW_NUMBER()' in sql_upper or 'PARTITION BY' in sql_upper:\n",
    "        categories.append('WINDOW_FUNCTION')\n",
    "    \n",
    "    # Aggregation types\n",
    "    if 'AVG(' in sql_upper:\n",
    "        categories.append('AVG')\n",
    "    if 'MAX(' in sql_upper:\n",
    "        categories.append('MAX')\n",
    "    if 'MIN(' in sql_upper:\n",
    "        categories.append('MIN')\n",
    "    if 'SUM(' in sql_upper:\n",
    "        categories.append('SUM')\n",
    "    if 'COUNT(' in sql_upper:\n",
    "        categories.append('COUNT')\n",
    "    \n",
    "    # Operators\n",
    "    if 'LESS_THAN' in sql or '<' in sql:\n",
    "        categories.append('LESS_THAN_OP')\n",
    "    if 'GREATER_THAN' in sql or '>' in sql:\n",
    "        categories.append('GREATER_THAN_OP')\n",
    "    if 'LESS_EQUAL' in sql or '<=' in sql:\n",
    "        categories.append('LESS_EQUAL_OP')\n",
    "    if 'GREATER_EQUAL' in sql or '>=' in sql:\n",
    "        categories.append('GREATER_EQUAL_OP')\n",
    "    \n",
    "    # Simple queries\n",
    "    if len(categories) == 0:\n",
    "        if 'SELECT * FROM' in sql_upper and 'WHERE' in sql_upper:\n",
    "            categories.append('SIMPLE_FILTER')\n",
    "        elif 'SELECT DISTINCT' in sql_upper:\n",
    "            categories.append('DISTINCT')\n",
    "        elif 'SELECT COUNT(*)' in sql_upper:\n",
    "            categories.append('SIMPLE_COUNT')\n",
    "        else:\n",
    "            categories.append('SIMPLE_SELECT')\n",
    "    \n",
    "    # Complexity level\n",
    "    complexity = 'SIMPLE'\n",
    "    if len(categories) >= 3:\n",
    "        complexity = 'COMPLEX'\n",
    "    elif len(categories) >= 2:\n",
    "        complexity = 'MEDIUM'\n",
    "    \n",
    "    return categories, complexity\n",
    "\n",
    "# Helper function to check if queries match semantically\n",
    "def queries_match_semantically(predicted, actual):\n",
    "    \"\"\"Check if two queries are semantically equivalent\"\"\"\n",
    "    # Exact match\n",
    "    if predicted.strip() == actual.strip():\n",
    "        return True, \"EXACT_MATCH\"\n",
    "    \n",
    "    # Normalize whitespace and compare\n",
    "    pred_normalized = ' '.join(predicted.split())\n",
    "    actual_normalized = ' '.join(actual.split())\n",
    "    if pred_normalized == actual_normalized:\n",
    "        return True, \"WHITESPACE_DIFF\"\n",
    "    \n",
    "    return False, \"DIFFERENT\"\n",
    "\n",
    "# Helper function to try executing and compare results\n",
    "def execute_and_compare(predicted_sql, actual_sql, conn):\n",
    "    \"\"\"Execute both queries and compare results\"\"\"\n",
    "    try:\n",
    "        pred_result = pd.read_sql_query(predicted_sql, conn)\n",
    "        pred_success = True\n",
    "    except Exception as e:\n",
    "        pred_result = None\n",
    "        pred_success = False\n",
    "        pred_error = str(e)\n",
    "    \n",
    "    try:\n",
    "        actual_result = pd.read_sql_query(actual_sql, conn)\n",
    "        actual_success = True\n",
    "    except Exception as e:\n",
    "        actual_result = None\n",
    "        actual_success = False\n",
    "        actual_error = str(e)\n",
    "    \n",
    "    if pred_success and actual_success:\n",
    "        # Compare results\n",
    "        if len(pred_result) == len(actual_result):\n",
    "            if pred_result.equals(actual_result):\n",
    "                return \"RESULTS_MATCH\", True\n",
    "            else:\n",
    "                return \"RESULTS_DIFFER\", False\n",
    "        else:\n",
    "            return f\"ROW_COUNT_DIFF ({len(pred_result)} vs {len(actual_result)})\", False\n",
    "    elif not pred_success and not actual_success:\n",
    "        return \"BOTH_FAILED\", False\n",
    "    elif not pred_success:\n",
    "        return f\"PRED_FAILED: {pred_error[:50]}\", False\n",
    "    else:\n",
    "        return f\"ACTUAL_FAILED: {actual_error[:50]}\", False\n",
    "\n",
    "# Analyze all test samples\n",
    "random.seed(42)\n",
    "all_indices = list(range(len(test_dataset)))\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nAnalyzing all test samples...\")\n",
    "for idx in all_indices:\n",
    "    if idx % 20 == 0:\n",
    "        print(f\"  Processing {idx}/{len(test_dataset)}...\")\n",
    "    \n",
    "    sample = test_dataset[idx]\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]]).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_length=MAX_TARGET_LENGTH)\n",
    "    \n",
    "    predicted_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    actual_sql = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "    \n",
    "    # Decode operators\n",
    "    predicted_sql = decode_sql_from_model(predicted_sql)\n",
    "    actual_sql = decode_sql_from_model(actual_sql)\n",
    "    \n",
    "    nl_question = test_data[idx][\"instruction\"]\n",
    "    \n",
    "    # Categorize\n",
    "    actual_categories, actual_complexity = categorize_query(actual_sql)\n",
    "    predicted_categories, predicted_complexity = categorize_query(predicted_sql)\n",
    "    \n",
    "    # Check if match\n",
    "    is_match, match_type = queries_match_semantically(predicted_sql, actual_sql)\n",
    "    \n",
    "    # Execute and compare\n",
    "    exec_result, results_match = execute_and_compare(predicted_sql, actual_sql, conn)\n",
    "    \n",
    "    results.append({\n",
    "        'idx': idx,\n",
    "        'question': nl_question,\n",
    "        'predicted_sql': predicted_sql,\n",
    "        'actual_sql': actual_sql,\n",
    "        'actual_categories': actual_categories,\n",
    "        'actual_complexity': actual_complexity,\n",
    "        'predicted_categories': predicted_categories,\n",
    "        'is_exact_match': is_match,\n",
    "        'match_type': match_type,\n",
    "        'exec_result': exec_result,\n",
    "        'results_match': results_match\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS - UPDATED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total = len(df_results)\n",
    "exact_matches = df_results['is_exact_match'].sum()\n",
    "result_matches = df_results['results_match'].sum()\n",
    "\n",
    "print(f\"\\nTotal test samples: {total}\")\n",
    "print(f\"Exact SQL matches: {exact_matches} ({100*exact_matches/total:.1f}%)\")\n",
    "print(f\"Semantic matches (same results): {result_matches} ({100*result_matches/total:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ACCURACY BY QUERY COMPLEXITY\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for complexity in ['SIMPLE', 'MEDIUM', 'COMPLEX']:\n",
    "    subset = df_results[df_results['actual_complexity'] == complexity]\n",
    "    if len(subset) > 0:\n",
    "        acc = subset['results_match'].sum()\n",
    "        print(f\"{complexity:12} : {acc}/{len(subset)} ({100*acc/len(subset):.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ACCURACY BY QUERY TYPE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Get all unique categories\n",
    "all_categories = set()\n",
    "for cats in df_results['actual_categories']:\n",
    "    all_categories.update(cats)\n",
    "\n",
    "category_stats = []\n",
    "for cat in sorted(all_categories):\n",
    "    subset = df_results[df_results['actual_categories'].apply(lambda x: cat in x)]\n",
    "    if len(subset) > 0:\n",
    "        correct = subset['results_match'].sum()\n",
    "        total_cat = len(subset)\n",
    "        accuracy = 100 * correct / total_cat\n",
    "        category_stats.append({\n",
    "            'category': cat,\n",
    "            'total': total_cat,\n",
    "            'correct': correct,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "\n",
    "df_cat_stats = pd.DataFrame(category_stats).sort_values('accuracy')\n",
    "print(df_cat_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"üéØ TARGET QUERY TYPES PERFORMANCE (After Retraining)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "target_types = {\n",
    "    'GREATER_EQUAL_OP': 'GREATER_EQUAL (>=)',\n",
    "    'LESS_EQUAL_OP': 'LESS_EQUAL (<=)',\n",
    "    'CTE': 'CTE Queries'\n",
    "}\n",
    "\n",
    "for key, label in target_types.items():\n",
    "    subset = df_cat_stats[df_cat_stats['category'] == key]\n",
    "    if len(subset) > 0:\n",
    "        row = subset.iloc[0]\n",
    "        print(f\"{label:25} : {row['correct']}/{row['total']} ({row['accuracy']:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{label:25} : No samples in test set\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOWEST PERFORMING QUERY TYPES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "worst_performing = df_cat_stats[df_cat_stats['accuracy'] < 85].sort_values('accuracy')\n",
    "if len(worst_performing) > 0:\n",
    "    print(worst_performing.to_string(index=False))\n",
    "    print(\"\\n‚ö†Ô∏è  These query types still need improvement!\")\n",
    "else:\n",
    "    print(\"‚úÖ All query types performing above 85%!\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SAMPLE FAILURES BY TYPE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Show examples of failures for each low-performing category\n",
    "for _, row in worst_performing.head(5).iterrows():\n",
    "    cat = row['category']\n",
    "    print(f\"\\nüìå Category: {cat} (Accuracy: {row['accuracy']:.1f}%)\")\n",
    "    \n",
    "    # Get a failure example\n",
    "    failures = df_results[\n",
    "        (df_results['actual_categories'].apply(lambda x: cat in x)) & \n",
    "        (~df_results['results_match'])\n",
    "    ]\n",
    "    \n",
    "    if len(failures) > 0:\n",
    "        example = failures.iloc[0]\n",
    "        print(f\"   Question: {example['question'][:80]}...\")\n",
    "        print(f\"   Expected: {example['actual_sql'][:100]}...\")\n",
    "        print(f\"   Got:      {example['predicted_sql'][:100]}...\")\n",
    "        print(f\"   Issue:    {example['exec_result']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED FAILURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "failures = df_results[~df_results['results_match']].copy()\n",
    "print(f\"\\nTotal failures: {len(failures)}\")\n",
    "\n",
    "if len(failures) > 0:\n",
    "    print(\"\\nFailure reasons:\")\n",
    "    failure_reasons = failures['exec_result'].value_counts()\n",
    "    for reason, count in failure_reasons.items():\n",
    "        print(f\"  {reason}: {count}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"SHOWING 10 RANDOM FAILURE EXAMPLES\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    sample_failures = failures.sample(min(10, len(failures)), random_state=42)\n",
    "    \n",
    "    for i, (_, row) in enumerate(sample_failures.iterrows(), 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FAILURE EXAMPLE {i} (Categories: {', '.join(row['actual_categories'])})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Question: {row['question']}\")\n",
    "        print(f\"\\nExpected SQL:\\n{row['actual_sql']}\")\n",
    "        print(f\"\\nPredicted SQL:\\n{row['predicted_sql']}\")\n",
    "        print(f\"\\nIssue: {row['exec_result']}\")\n",
    "        print(f\"Complexity: {row['actual_complexity']}\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Save detailed results to CSV\n",
    "output_csv = \"updated_model_test_analysis.csv\"\n",
    "df_results.to_csv(output_csv, index=False)\n",
    "print(f\"\\n‚úÖ Detailed results saved to {output_csv}\")\n",
    "\n",
    "# Save category statistics\n",
    "cat_stats_csv = \"updated_model_category_performance.csv\"\n",
    "df_cat_stats.to_csv(cat_stats_csv, index=False)\n",
    "print(f\"‚úÖ Category statistics saved to {cat_stats_csv}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE FOR UPDATED MODEL!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model tested: {MODEL_DIR}\")\n",
    "print(f\"Compare these results with your original model to see improvements!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc230b2-9fca-45ce-9580-8d20a1703c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"customer_data.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"PRAGMA table_info(customer_demographics);\")\n",
    "print(cursor.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb8b94-baa2-4b9d-896d-5c8206381e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
